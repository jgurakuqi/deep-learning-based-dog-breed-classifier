{"cells":[{"cell_type":"markdown","metadata":{},"source":["In this kernel I will use a pre-trained Inception V3 as feature extractor of my CNN model and train it further (along with my custom fully-connected NN classifier), to make it able to classify the breed of any dog given in the input image with an high accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from torchvision import transforms, datasets, models\n","\n","from torch import optim, cuda, device, max, mean, FloatTensor, save, no_grad, backends, load, autograd\n","from torch.utils.data import DataLoader, sampler, random_split\n","import torch.nn as nn\n","\n","# from PIL import Image\n","from PIL.Image import open\n","from numpy import random, zeros\n","from pandas import DataFrame\n","from os import listdir, mkdir\n","from matplotlib.pyplot import figure, plot, subplot, imread, imshow\n","%matplotlib inline\n","\n","import xml.etree.ElementTree as ET\n","\n","cuda.init()\n","cuda.empty_cache()\n","cuda.memory_summary(device=None, abbreviated=False)\n","\n","def set_debug_apis(state: bool = False):\n","    autograd.profiler.profile(enabled=state)\n","    autograd.profiler.emit_nvtx(enabled=state)\n","    autograd.set_detect_anomaly(mode=state)\n","\n","# Then in training code before the train loop \n","set_debug_apis(state=False)"]},{"cell_type":"markdown","metadata":{},"source":["After importing the necessary modules, we will define a set of functions useful for taking (and printing) a sample of data from the dataset and then transforming it to conform to the standard Inception V3 input (pre-trained), also defining the online data increments that I will use during training. \n","\n","First of all, I'll print the samples along with their cropped versions."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def crop_image(breed, dog, data_dir):\n","    tree = ET.parse(data_dir + \"annotation/\" + breed + \"/\" + dog)\n","    bounding_box = tree.getroot().findall(\"object\")[0].find(\"bndbox\")\n","    xmin = int(bounding_box.find(\"xmin\").text)\n","    xmax = int(bounding_box.find(\"xmax\").text)\n","    ymin = int(bounding_box.find(\"ymin\").text)\n","    ymax = int(bounding_box.find(\"ymax\").text)\n","    return imread(data_dir + \"images/\" + breed + \"/\" + dog + \".jpg\")[\n","        ymin:ymax, xmin:xmax, :\n","    ]\n","\n","\n","data_dir = \"../data/\"\n","breed_list = listdir(data_dir + \"images/\")\n","running_device = device(\"cuda:0\" if cuda.is_available() else \"cpu\")\n","print(running_device)\n","\n","\n","def plot_sample_image_vs_crop(data_dir):\n","    figure(figsize=(20, 20))\n","    for i in range(4):\n","        subplot(421 + (i * 2))\n","        breed = random.choice(breed_list)\n","        dog = random.choice(listdir(data_dir + \"annotation/\" + breed))\n","        img = imread(data_dir + \"images/\" + breed + \"/\" + dog + \".jpg\")\n","        imshow(img)\n","\n","        tree = ET.parse(data_dir + \"annotation/\" + breed + \"/\" + dog)\n","        boundingBox = tree.getroot().findall(\"object\")[0].find(\"bndbox\")\n","        xmin = int(boundingBox.find(\"xmin\").text)\n","        xmax = int(boundingBox.find(\"xmax\").text)\n","        ymin = int(boundingBox.find(\"ymin\").text)\n","        ymax = int(boundingBox.find(\"ymax\").text)\n","        plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin])\n","        crop_img = crop_image(breed, dog, data_dir)\n","        subplot(422 + (i * 2))\n","        imshow(crop_img)\n","\n","\n","plot_sample_image_vs_crop(data_dir=data_dir)\n"]},{"cell_type":"markdown","metadata":{},"source":["Now I'will create a new folder to store the cropped version of the images from the dataset, in order to use them for the training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_image(breed_and_file):\n","    global data_dir\n","    img = open(\"\".join([data_dir, \"images/\", breed_and_file, \".jpg\"]))\n","    tree = ET.parse(\"\".join([data_dir, \"annotation/\", breed_and_file]))\n","    boundingBox = tree.getroot().findall(\"object\")[0].find(\"bndbox\")\n","    xmin = int(boundingBox.find(\"xmin\").text)\n","    xmax = int(boundingBox.find(\"xmax\").text)\n","    ymin = int(boundingBox.find(\"ymin\").text)\n","    ymax = int(boundingBox.find(\"ymax\").text)\n","    img = img.crop((xmin, ymin, xmax, ymax))\n","    img = img.convert(\"RGB\")\n","    img.save(\"\".join([data_dir, \"cropped_images/\", breed_and_file, \".jpg\"]))\n","\n","\n","def parallel_save_cropped_images(data_dir, breed_list):\n","    from itertools import chain\n","    from multiprocessing import Pool\n","    with Pool() as pool:\n","        pool.map(\n","            func=save_image,\n","            iterable=list(\n","                chain.from_iterable(\n","                    [\n","                        [\n","                            \"\".join([breed, \"/\", file])\n","                            for file in listdir(\n","                                \"\".join([data_dir, \"annotation/\", breed])\n","                            )\n","                        ]\n","                        for breed in breed_list\n","                    ]\n","                )\n","            ),\n","            chunksize=50,\n","        )\n","\n","\n","if \"cropped_images\" not in listdir(data_dir):\n","    mkdir(data_dir + \"cropped_images/\")\n","    for breed in breed_list:\n","        mkdir(data_dir + \"cropped_images/\" + breed)\n","    print(\n","        \"Created {} folders to store cropped images of the different breeds.\".format(\n","            len(listdir(data_dir))\n","        )\n","    )\n","    parallel_save_cropped_images(data_dir, breed_list)\n","    print(\"Saved cropped images in said folders.\")"]},{"cell_type":"markdown","metadata":{},"source":["Now that all cropped images are in place, I'll define the data augmentation needed to improve the generalisation capabilities of my model and the normalisations required by the pre-trained feature extractor (resized to 299 * 299 and normalized as well according to the ImageNet standards).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","image_transforms = {\n","    # Train uses data augmentation\n","    \"train\": A.Compose(\n","        [\n","            A.RandomRotate90(),\n","            A.Flip(),\n","            A.Transpose(),\n","            A.GaussNoise(p=0.2),\n","            A.OneOf([\n","                A.MotionBlur(p=.2),\n","                A.MedianBlur(blur_limit=3, p=0.1),\n","                A.Blur(blur_limit=3, p=0.1),\n","            ], p=0.2),\n","            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n","            A.OneOf([\n","                A.OpticalDistortion(p=0.3),\n","                A.GridDistortion(p=.1),\n","                A.PiecewiseAffine(p=0.3),\n","            ], p=0.2),\n","            A.OneOf([\n","                A.CLAHE(clip_limit=2),\n","                A.Sharpen(),\n","                A.Emboss(),\n","                A.RandomBrightnessContrast(),            \n","            ], p=0.3),\n","            A.HueSaturationValue(p=0.3),\n","            A.CenterCrop(height=299, width=299, always_apply=True), # Image net standards\n","            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # Imagenet standards\n","            ToTensorV2(),\n","        ]\n","    ),\n","    \"test\": transforms.Compose(\n","        [\n","            transforms.Resize(size=299),\n","            transforms.CenterCrop(size=299),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","        ]\n","    ),\n","}"]},{"cell_type":"markdown","metadata":{},"source":["Now I will define the batch-size (60 is the maximum size tollarated by my GPU's VMEM before of going into a OOM exception), and then I will prepare the dataloaders for the training and testing, splitting the data in the following manner: 80% training, 10% testing and 10% validation."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 60 # Previous working :Values: 16, 32, 40, 50, 58 - BEST 60\n","\n","all_data = datasets.ImageFolder(root=data_dir + \"cropped_images/\")\n","train_data_len = int(len(all_data) * 0.8)\n","valid_data_len = int((len(all_data) - train_data_len) / 2)\n","test_data_len = int(len(all_data) - train_data_len - valid_data_len)\n","train_data, val_data, test_data = random_split(\n","    all_data, [train_data_len, valid_data_len, test_data_len]\n",")\n","train_data.dataset.transform = image_transforms[\"train\"]\n","val_data.dataset.transform = image_transforms[\"test\"]\n","test_data.dataset.transform = image_transforms[\"test\"]\n","print(len(train_data), len(val_data), len(test_data))\n","\n","train_loader = DataLoader(\n","    train_data, num_workers=4, pin_memory=True, batch_size=batch_size, shuffle=True\n",")\n","val_loader = DataLoader(\n","    val_data, num_workers=4, pin_memory=True, batch_size=batch_size, shuffle=True\n",")\n","test_loader = DataLoader(\n","    test_data, num_workers=4, pin_memory=True, batch_size=batch_size, shuffle=True\n",")\n","\n","trainiter = iter(train_loader)\n","features, labels = next(trainiter)\n","print(features.shape, labels.shape)"]},{"cell_type":"markdown","metadata":{},"source":["I now instantiate the model by loading it with pre-trained weights, because it trains faster and achieves better results. Specifically for the Inception model, we'll have to set the aux_logits property to False, otherwise the auxilary classifier included in the Inception v3 model will try to enforce its regularisation, which is unnecessary considering that I use just the feature extraction part of the said model. We can give a look to the model architecture:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1) # or None for no trained weights\n","model.aux_logits = False\n","model"]},{"cell_type":"markdown","metadata":{},"source":["I want to make sure that all the layers of the model will be trainable, because I achieved better results wrt them being freezed."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for param in model.parameters():\n","    param.requires_grad = True"]},{"cell_type":"markdown","metadata":{},"source":["Now I define the Fully connected 2 layer classifier to concatenate at the end of the feature extractor."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_classes = 120\n","n_inputs = model.fc.in_features\n","model.fc = nn.Sequential(\n","    nn.Linear(n_inputs, 1024),\n","    nn.ReLU(),\n","    nn.Dropout(0.4),\n","    nn.Linear(1024, n_classes),\n","    nn.LogSoftmax(dim=1)).to(running_device)\n","model.fc"]},{"cell_type":"markdown","metadata":{},"source":["Now I load the entire model on the GPU to accelarate the training and I enable the CUDA Benchmark option which after some running it looks for a more efficient way to perform the calculations.\n","Eventually I choose also the loss function that I will use during the training to compute error rates. In this case I chose an NNLLoss to use toghether with a LogSoftmax (an alternative could have been Softmax + CrossEntropyLoss)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if cuda.is_available():\n","    model.cuda()\n","    print(\"-- Cuda enabled --\")\n","    backends.cudnn.benchmark = True # BENCHMARK FOR SPEED UP---------https://pytorch.org/docs/stable/backends.html#torch-backends-cudnn\n","criterion = nn.NLLLoss()"]},{"cell_type":"markdown","metadata":{},"source":["Creating the mapping index to class and class to index for my model, making it able to distinguish the predicted classes."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.class_to_idx = all_data.class_to_idx  # all_data.class_to_idx\n","model.idx_to_class = {idx: class_ for class_, idx in model.class_to_idx.items()}\n","\n","\n","def print_class_indexes(raw_indexes_to_classes):\n","    from numpy import array\n","    import pandas as pd\n","\n","    all_classes = array(list(raw_indexes_to_classes))\n","    all_classes_frame = DataFrame(\n","        {\n","            \"class_name\": all_classes[:, 1],\n","        }\n","    )\n","    print(all_classes_frame)\n","\n","\n","print_class_indexes(model.idx_to_class.items())\n","\n","del (print_class_indexes,)\n","del breed_list, n_classes, n_inputs, features, image_transforms, all_data, val_data"]},{"cell_type":"markdown","metadata":{},"source":["Finally I import the modules containing my training and utility functions, and I start the training with a function which will look for the best possible model by training as much models as specified by me, using an array of precomputed learning rates to train each of such models. \n","The training will be interrupted if the early stop condition is met (I chose to stop after 3 epochs without improvement because it usually fastens the training and avoids overfitting).\n","The pre-calculated lr were obtained with a similar training function, which searches for the best possible model in a similar way, calculating a very large amount of lr by means of a gamma array, so that I can work out the 'direction' in which I can obtain a very good model, and I can use the lr that led to that direction for further runs on the previous training function using a more re-extracted and granular range of learning rates that will revolve around the learning rates obtained from the second function.\n","\n","I chose an SGD as optimizer for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from training_utils import train, send_telegram_message\n","from numpy import arange\n","import traceback\n","from pruning_based_training_utils import (\n","    pruning_based_training,\n","    pruning_based_training_with_precomputed_lrs,\n",")\n","import importlib\n","import pruning_based_training_utils\n","\n","importlib.reload(pruning_based_training_utils)\n","from pruning_based_training_utils import (\n","    pruning_based_training,\n","    pruning_based_training_with_precomputed_lrs,\n",")\n","\n","scheduler = None\n","\n","try:\n","    send_telegram_message(\n","        \"\".join(\n","            [\n","                \"///////////////////////////////////////////////\\n\",\n","                \"///////////////////////////////////////////////\\n\",\n","                \"///////////////////////////////////////////////\\n\",\n","                \"CHOSEN SCHEDULER: \",\n","                type(scheduler).__name__,\n","                \"\\n\\n-----------------STARTING TRAINING-----------------\",\n","            ]\n","        )\n","    )\n","    optimizer = optim.SGD(\n","        model.parameters(), lr=0.0074, momentum=0.9, weight_decay=1e-6\n","    )\n","    gamma_range = arange(0.01, 1.1, 0.025)\n","    precomputed_lrs = [\n","        arange(0.00647, 0.006471, 0.001),\n","        arange(0.002145, 0.002155, 0.0000025),\n","        arange(0.00200, 0.00210, 0.000025),\n","        arange(0.0009224, 0.00092325, 0.00000025),\n","        arange(0.00012143, 0.000121447, 0.0000000015),  # 000124571\n","        arange(0.0000755, 0.0000770, 0.0000005),\n","        arange(0.00003125, 0.00003140, 0.00000005),\n","    ]\n","    model, history, performance_dict = pruning_based_training_with_precomputed_lrs(\n","        original_model=model,\n","        criterion=criterion,\n","        original_optimizer=optimizer,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        early_stop=3,\n","        n_epochs=400,\n","        precomputed_lrs=precomputed_lrs,\n","        base_gammas=gamma_range,\n","    )  # 87.3 % TEST | 87.07% VALID | Validation Loss: 0.4311\n","    send_telegram_message(\n","        \"\".join(\n","            [\n","                \"---TRAINING COMPLETED---\\n\",\n","                \"BEST EPOCH: \",\n","                str(performance_dict[\"best_epoch\"]),\n","            ]\n","        )\n","    )\n","except Exception as ex:\n","    error_message = \"\".join(\n","        [\"--- CNN EXECUTION ERROR! ---:\\n\", str(ex), \"\\n\", traceback.format_exc()]\n","    )\n","    print(error_message)\n","    send_telegram_message(error_message)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history"]},{"cell_type":"markdown","metadata":{},"source":["I can now test the obtained model and see if it does better than the currently best overall model, saved in the memory as a checkpoint, and eventually overwriting it.\n","The best model until know achieved a 87.3% test accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def test(model, test_loader, criterion):\n","    with no_grad():\n","        model.eval()\n","        test_acc = 0\n","        for data, label in test_loader:\n","            data, label = data.cuda(), label.cuda()\n","\n","            output = model(data)\n","\n","            _, pred = max(output, dim=1)\n","            correct_tensor = pred.eq(label.data.view_as(pred))\n","            accuracy = mean(correct_tensor.type(FloatTensor))\n","            test_acc += accuracy.item() * data.size(0)\n","\n","        test_acc = test_acc / len(test_loader.dataset)\n","        return test_acc\n","\n","\n","def load_model_from_checkpoint(path, load_also_accuracy=False):\n","    model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n","    n_classes = 120\n","    n_inputs = model.fc.in_features\n","    model.aux_logits = False\n","    model.fc = nn.Sequential(\n","        nn.Linear(n_inputs, 1024),\n","        nn.ReLU(),\n","        nn.Dropout(0.4),\n","        nn.Linear(1024, n_classes),\n","        nn.LogSoftmax(dim=1),\n","    )\n","    test_accuracy = 0\n","    try:\n","        torch_checkpoint = load(path)\n","        model.load_state_dict(torch_checkpoint[\"model_state_dict\"])\n","        model.idx_to_class = torch_checkpoint[\"idx_to_class\"]\n","        if load_also_accuracy:\n","            test_accuracy = torch_checkpoint[\"test_acc\"]\n","        # model.cuda()\n","    except:\n","        return None, None\n","    return model, test_accuracy\n","\n","\n","def save_model(model_to_save, test_accuracy):\n","    save(\n","        {\n","            \"model_state_dict\": model_to_save.state_dict(),\n","            \"idx_to_class\": model_to_save.idx_to_class,\n","            \"test_acc\": test_accuracy,\n","        },\n","        \"best_dog_classifier_model.pt\",\n","    )\n","\n","\n","def compare_models(model, test_loader, criterion, save=True):\n","    save_threshold = 84.0\n","    loaded_model, loaded_model_test_acc = load_model_from_checkpoint(\n","        path=\"best_dog_classifier_model.pt\", load_also_accuracy=True\n","    )\n","    test_acc = test(model, test_loader, criterion) * 100\n","    message = f\"Current model test accuracy: {test_acc}.\\n\"\n","    if loaded_model == None:\n","        message += \"No model stored found.\\n\"\n","        if test_acc > save_threshold:\n","            save_model(model_to_save=model, test_accuracy=test_acc)\n","            message += f\"The accuracy of the new model overcomes the {save_threshold:.2f} save threshold.\\nNew model saved.\\n\"\n","        else:\n","            message += f\"The accuracy of the new model DOES NOT overcome the {save_threshold:.2f} save threshold.\\n\"\n","    else:\n","        if test_acc > loaded_model_test_acc:\n","            save_model(model_to_save=model, test_accuracy=test_acc)\n","            message += f\"The accuracy of the new model overcomes the accuracy of the saved model.\\nNew model saved.\\n\"\n","        else:\n","            message += f\"The accuracy of the new model DOES NOT overcome the accuracy of the saved model.\\nAccuracy of the save model: {loaded_model_test_acc}\\n\"\n","    print(message)\n","    send_telegram_message(message)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_to_check, _ = load_model_from_checkpoint(\n","    path=\"pruning_best_global_model.pt\", load_also_accuracy=False\n",")\n","\n","compare_models(\n","    model=model_to_check.cuda(),\n","    test_loader=test_loader,\n","    criterion=criterion,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy achieved is very good considering that some dog breeds can be extremely similar and that we ourselves may not be able to distinguish them with our eyes, due to very subtle differential characteristics such as fur colours."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate(model, test_loader):\n","\n","    classes = []\n","    acc_results = zeros(len(test_loader.dataset))\n","    i = 0\n","    model.eval()\n","    with no_grad():\n","        for data, labels in test_loader:\n","            data, labels = data.cuda(), labels.cuda()\n","            output = model(data)\n","            for pred, true in zip(output, labels):\n","                _, pred = pred.unsqueeze(0).topk(1)\n","                correct = pred.eq(true.unsqueeze(0))\n","                acc_results[i] = correct.cpu()\n","                classes.append(model.idx_to_class[true.item()][10:])\n","                i += 1\n","    results = DataFrame({\"class\": classes, \"results\": acc_results})\n","    results = results.groupby(classes).mean()\n","\n","    return results\n","\n","evaluate(model_to_check, test_loader)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
